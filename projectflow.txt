1. Create a new github public repo and clone it in local.
2. Create your template.py file and run it.
3. Setup your conda venv:
    * conda create -n chest python=3.8 -y
    * conda activate chest
    * pip install -r requirements.txt
4. In this project we will write code our logging within the src.cnnClassifier.__init__.py file
5. Test your logging setup on demo.py file
6. Now add the src.cnnClassifier.utils.common.py file
7. Hands on with Ensure and Configbox module on research.trials.ipynb file
8. Inside "research" dir, create one "mlflow_demo.py" and "argv-demo.py" file for exp purpose.
9. Steps to execute mlflow_demo.py:
    * Create an account on Dagshub, connect it to your Github
    * On Dagshub dashboard top right corner > create > new repository > connect a repo
      > Github connect > Add/revoke access to repo > Browse Github and select the repo
      > Connect repository > Remote > Experiments > Copy all commands below "Using MLFlow tracking"
    * Now you need to set 3 env variables using bash or powershell terminal (bash recommended)
    * For bash terminal:
        export MLFLOW_TRACKING_URI=https://dagshub.com/vikashdasXXX/MLflow-XXXXXX-Demo.mlflow
        export MLFLOW_TRACKING_USERNAME=vikashdasXXX
        export MLFLOW_TRACKING_PASSWORD=f4f3992ed3aXXXXXXXX4b3983126b4b9
    * For powershell terminal:
        $env:MLFLOW_TRACKING_URI = "https://dagshub.com/vikashdasXXX/MLflow-XXXXXX-Demo.mlflow"
        $env:MLFLOW_TRACKING_USERNAME = "vikashdasxxx"
        $env:MLFLOW_TRACKING_PASSWORD = "f4f3992ed3aXXXXXXXX4b3983126b4b9"
    * Now change the remote_server_uri var from mlflow_demo.py file
    * Run from bash terminal: "python mlflow_demo.py" (make sure venv is active on bash as well)
    * After excecution is completed, go back to Dagshub screen > Remote > Go to mlflow UI > Table
      and you will be able to see all the experiments performed so far (eg 5 exp if you executed
      the mlflow_demo.py file 5times)
    * You can also run the mlflow_demo.py file as "python mlflow_demo.py 0.2 0.2" (0.5 will 
      be taken by the sys if no arguments given manually)
10. Now we start with the Data Ingestion component:
    * Upload the zipped data first on your gdrive
    * Try downloading the data once from trails.ipynb file.
11. Work on data ingestion notebook for experimenting. 
    (config.yaml, constant, common.py, params.yaml[dummy value] should be ready and add "artifacts/*" to .gitignore)
12. Work on first component "Data Ingestion" as per workflow mentioned below.
13. Work on notebook - "02_prepare_base_model.ipynb" file.
14. Add the "prepare_base_model" component as per the workflows.
15. Work on notebook - "03_model_trainer.ipynb" file.
16. Add the "model_trainer" component as per the workflows.
17. Work on notebook - "04_model_evaluation_with_mlflow.ipynb" file:
    * First, connect your igthub repo with Dagshub (as done earlier)
    * Set env var on bash terminal(shown above) and via notebook as well.
    * Execute as per notebook code flow.
18. Add the "model_evaluation" component as per the workflows and choose params 
    by doing experiments with the help of mlflow.
19. Setting up the dvc.yaml file then run pipeline using it.
    * dvc init (this will create 2 dir: .dvc and .dvcignore)
    * dvc repro (this will create 2 dir: dvc.lock and scores.json)
    *Observation*: Initially when I tried "dvc repro", it gave me an error as it was 
    looking for a file that already exists inside .dvc/cache, upon investigating I realised 
    that my root dir name was really long so I renamed it with a shorter name. Then I deleted .dvc
    and .dvcignore dir to perform "dvc init" again. I also had to re-activate my venv and did
    "pip install -r requirements.txt" again. Previous error was gone but again got an unexpected
    error during execution of 3rd stage, I could'nt figure out the reason so I just re-ran the 
    pipeline by "dvc repro" and it helped.
20. Moving to "Prediction" pipeline:
    * First create one "model" dir inside root and paste final model inside.
    * Setup the "prediction.py" file insdie cnnClassifier.pipeline
21. Create "templates" dir and add html file inside, also work on app.py
22. For Containerization, create files: Dockerfile, docker-compose.yml, .dockerignore
23. CICD:
    * Create new dir: ".jenkins"
    * Inside .jenkins, create a file named "Jenkinsfile"
    * Commit and push the code once "Jenkinsfile" is ready.
    * Go to Github repo, open "docker-compose.yml" file and click on 'raw' > copy the url.
    * Now come back to Jenkinsfile and paste the url within 'Continuous Deployment' stage.
24. Scripts dir added with all the scripts required for running jenkins inside aws server
    and to setup AWS instance.
25. Next we will work on the master-slave architechture of Jenkins. Basically we will create 
    2 AWS instances. One will do the CI and containerization thing and push it to ECR, another
    will pull image from ECR and do the CD (continuous deployment) part.
    * Login to AWS console and create one IAM user > name: "chest-user" > Attach Policy Directly
      > AdministratorAccess > next > Create User > Go to user > Security Credentials > Access Key
      > Create Access Key > CLI > select condition box > next > Create Access Key > Download csv file
    * Creating one EC2 instance for Jenkins: Launch instance > name: "jenkins-machine" > AMI: Ubuntu
      > Ubuntu Server 20.04 LTS > t2.medium (4GB RAM) > create new key-pair > name: "jenkins-key"
      > RSA > .pem > create key pair > save > allow http/https traffic > config storage: 30GB > launch
      > go to instance id > connect > Connect using EC2 Instance Connect > Connect
    * Now we need to setup jenkins on this ec2 instance: open the scripts.jenkins.sh file
    * Setting up an Elastic IP: go to Elastic IP Addresses > Allocate Elastic IP Address 
      > keeping setting as default, click on allocate > Associate this Elastic IP Address > select your ec2
      > click Associate
    * By default Jenkins will run on port 8080 that we need to add to our ec2 instance:
      > go to ec2 > Security > go to security groups > inbound rules > edit inbound rules > add rule
      > custom tcp > 8080 > 0.0.0.0/0 > save rules
    * Now copy the public ip address of the ec2 instance and paste it in address bar followed by :8080
      > now refer back to jenkins.sh for administrator password
    * We will also need an ECR repo: Go to ECR > Get started > private > name: chest > Create Repo
      > Copy the ECR URI and keep it.
    * Now we need to set the secret variables on Jenkinsfile:
      > On Jenkins dashboard, go to "manage jenkins" > Security > Credentials > click on 'system'
      > click on 'global credentials' > Add Credentials > Kind: Secret Text > ID: ECR_REPOSITORY
      > Secret: previously copied ECR URI > Create > Similarly click 'Add Credentials' and add it for
      AWS_ACCOUNT_ID, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
      > Similar way we also need to setup the ssh key but with few changes: Kind: SSH Username with private key
      > scope: global > id: ssh_key > private key: enter directly > add > copy paste the value from jenkins-key.pem
      > create > (Now we need to setup a plugin for the ssh key) > go to jenkins dashboard > manage jenkins
      > plugins > available plugins > search for 'ssh' > select 'ssh agent' > install > scroll to the bottom
      > select: Restart Jenkins when installation is complete and no jobs are running
      > login back to jenkins using credentials
    * Now we need to create a pipeline on Jenkins so that we can perform CICD:
      > Jenkins dashboard > new item > name: chest-project > select pipeline > ok > scroll to bottom 
      > definition: Pipeline Script from SCM > SCM: git > Repo URL: https url of repo from git 
      > Branch specifier: main/master > Script path: .jenkins/Jenkinsfile > Save
    * Now we will create our next ec2 instance for CD:
      ec2 > launch instance > name: chest-machine > AMI: Ubuntu > ubuntu server 20.04 LTS > t2.large (8GB)
      > key pair login: select jenkin-key that we created earlier > allow http/https traffic > storage: 32GB
      > launch instance > go inside instance once created > connect > Connect using EC2 Instance Connect > connect
      > once you get into the instance terminal, refer to scripts.ec2_setup.sh
    * Now everytime we push our code to Github, it should trigger the CICD pipeline via jenkins therefore we need
      a workflow hence .github\workflows\main.yaml file added and the secrets needs to be added to Github:
      Go to github project > settings > security > secret and variables > action > New Repo Secret > Name: URL
      > Secret: (url of jenkins dashboard page[eg http://34.228.104.93:8080/]) > Add Secret
      > New Repo Secret > Name: USER > Secret: vikashdas770 > Add Secret
      > New Repo Secret > Name: TOKEN > Secret: (go to jenkins dashboard > people > select your userid > configure
      > add new token > generate > copy the token > paste) > Add Secret
      > New Repo Secret > Name: JOBS > Secret: chest-project (check on jenkins dashboard) > Add Secret
    * Now push the code from local to git. Go to github coz we need to manually trigger the workflow:
      > actions > Trigger Jenkins Job (on left hand) > run workflow > run workflow
    * Go to jenkins dashboard now to see the CICD pipeline running. Once CICD is complete, move to next step.
    * Add 8080 port to chest-machine instance (the way we did earlier) and see deployment at publicip:8080
26. Finally delete all your AWS resources created for this project (EC2*2, IAM, ECR, Elastic IP)

## Workflows
* Update config.yaml
* Update params.yaml
* Update the entity
* Update the configuration manager in src.config.configuration.py
* Update the components
* Update the pipeline
* Update the main.py
* Update the dvc.yaml