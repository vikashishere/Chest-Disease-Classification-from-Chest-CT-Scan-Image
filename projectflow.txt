1. Create a new github public repo and clone it in local.
2. Create your template.py file and run it.
3. Setup your conda venv:
    * conda create -n chest python=3.8 -y
    * conda activate chest
    * pip install -r requirements.txt
4. In this project we will write code our logging within the src.cnnClassifier.__init__.py file
5. Test your logging setup on demo.py file
6. Now add the src.cnnClassifier.utils.common.py file
7. Hands on with Ensure and Configbox module on research.trials.ipynb file
8. Inside "research" dir, create one "mlflow_demo.py" and "argv-demo.py" file for exp purpose.
9. Steps to execute mlflow_demo.py:
    * Create an account on Dagshub, connect it to your Github
    * On Dagshub dashboard top right corner > create > new repository > connect a repo
      > Github connect > Add/revoke access to repo > Browse Github and select the repo
      > Connect repository > Remote > Experiments > Copy all commands below "Using MLFlow tracking"
    * Now you need to set 3 env variables using bash or powershell terminal (bash recommended)
    * For bash terminal:
        export MLFLOW_TRACKING_URI=https://dagshub.com/vikashdasXXX/MLflow-XXXXXX-Demo.mlflow
        export MLFLOW_TRACKING_USERNAME=vikashdasXXX
        export MLFLOW_TRACKING_PASSWORD=f4f3992ed3aXXXXXXXX4b3983126b4b9
    * For powershell terminal:
        $env:MLFLOW_TRACKING_URI = "https://dagshub.com/vikashdasXXX/MLflow-XXXXXX-Demo.mlflow"
        $env:MLFLOW_TRACKING_USERNAME = "vikashdasxxx"
        $env:MLFLOW_TRACKING_PASSWORD = "f4f3992ed3aXXXXXXXX4b3983126b4b9"
    * Now change the remote_server_uri var from mlflow_demo.py file
    * Run from bash terminal: "python mlflow_demo.py" (make sure venv is active on bash as well)
    * After excecution is completed, go back to Dagshub screen > Remote > Go to mlflow UI > Table
      and you will be able to see all the experiments performed so far (eg 5 exp if you executed
      the mlflow_demo.py file 5times)
    * You can also run the mlflow_demo.py file as "python mlflow_demo.py 0.2 0.2" (0.5 will 
      be taken by the sys if no arguments given manually)
10. Now we start with the Data Ingestion component:
    * Upload the zipped data first on your gdrive
    * Try downloading the data once from trails.ipynb file.
11. Work on data ingestion notebook for experimenting. 
    (config.yaml, constant, common.py, params.yaml[dummy value] should be ready and add "artifacts/*" to .gitignore)
12. Work on first component "Data Ingestion" as per workflow mentioned below.
13. Work on notebook - "02_prepare_base_model.ipynb" file.
14. Add the "prepare_base_model" component as per the workflows.
15. Work on notebook - "03_model_trainer.ipynb" file.
16. Add the "model_trainer" component as per the workflows.
17. Work on notebook - "04_model_evaluation_with_mlflow.ipynb" file:
    * First, connect your igthub repo with Dagshub (as done earlier)
    * Set env var on bash terminal(shown above) and via notebook as well.
    * Execute as per notebook code flow.
18. Add the "model_evaluation" component as per the workflows and choose params 
    by doing experiments with the help of mlflow.
19. Setting up the dvc.yaml file then run pipeline using it.
    * dvc init (this will create 2 dir: .dvc and .dvcignore)
    * dvc repro (this will create 2 dir: dvc.lock and scores.json)
    *Observation*: Initially when I tried "dvc repro", it gave me an error as it was 
    looking for a file that already exists inside .dvc/cache, upon investigating I realised 
    that my root dir name was really long so I renamed it with a shorter name. Then I deleted .dvc
    and .dvcignore dir to perform "dvc init" again. I also had to re-activate my venv and did
    "pip install -r requirements.txt" again. Previous error was gone but again got an unexpected
    error during execution of 3rd stage, I could'nt figure out the reason so I just re-ran the 
    pipeline by "dvc repro" and it helped.
20. Moving to "Prediction" pipeline:
    * First create one "model" dir inside root and paste final model inside.
    * Setup the "prediction.py" file insdie cnnClassifier.pipeline
21. Create "templates" dir and add html file inside, also work on app.py
22. For Containerization, create files: Dockerfile, docker-compose.yml, .dockerignore
23. CICD:
    * Create new dir: ".jenkins"
    * Inside .jenkins, create a file named "Jenkinsfile"
    * Commit and push the code once "Jenkinsfile" is ready.




## Workflows
* Update config.yaml
* Update params.yaml
* Update the entity
* Update the configuration manager in src.config.configuration.py
* Update the components
* Update the pipeline
* Update the main.py
* Update the dvc.yaml